{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data-Generation.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "9m-earbwEn7v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ]
    },
    {
      "metadata": {
        "id": "OG0q5Ea4Efzp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageDraw\n",
        "import numpy as np\n",
        "from sklearn import datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j9MBJh1iEsyQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ]
    },
    {
      "metadata": {
        "id": "S3XvghhYEs-U",
        "colab_type": "code",
        "outputId": "63aab36c-0828-46a4-b93f-4a713f44559f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        }
      },
      "cell_type": "code",
      "source": [
        "# Mount CU Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir drive/My\\ Drive/synthetic-ds\n",
        "!mkdir drive/My\\ Drive/synthetic-ds/ct2\n",
        "!mkdir drive/My\\ Drive/synthetic-ds/ctp2\n",
        "!mkdir drive/My\\ Drive/synthetic-ds/squares\n",
        "!mkdir drive/My\\ Drive/synthetic-ds/point\n",
        "!mkdir drive/My\\ Drive/synthetic-ds/point/circles\n",
        "!mkdir drive/My\\ Drive/synthetic-ds/point/s_curves\n",
        "!mkdir drive/My\\ Drive/synthetic-ds/point/swiss_roll"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "mkdir: cannot create directory ‘drive/My Drive/synthetic-ds’: File exists\n",
            "mkdir: cannot create directory ‘drive/My Drive/synthetic-ds/ct2’: File exists\n",
            "mkdir: cannot create directory ‘drive/My Drive/synthetic-ds/ctp2’: File exists\n",
            "mkdir: cannot create directory ‘drive/My Drive/synthetic-ds/squares’: File exists\n",
            "mkdir: cannot create directory ‘drive/My Drive/synthetic-ds/point’: File exists\n",
            "mkdir: cannot create directory ‘drive/My Drive/synthetic-ds/point/circles’: File exists\n",
            "mkdir: cannot create directory ‘drive/My Drive/synthetic-ds/point/s_curves’: File exists\n",
            "mkdir: cannot create directory ‘drive/My Drive/synthetic-ds/point/swiss_roll’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Cb5sZYASEwVT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Image Dataset Generation"
      ]
    },
    {
      "metadata": {
        "id": "cO6nsN7DMQ3n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Squares"
      ]
    },
    {
      "metadata": {
        "id": "-qiaStMpMRFx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_sample(mean, stdev):\n",
        "    return np.random.multivariate_normal(mean, (stdev**2)*np.identity(2))\n",
        "\n",
        "def gen_mixture(no_of_dist, dataset_size, scaling_factor = (100, 10)):\n",
        "    # Generate means and stdev of the mixture\n",
        "    mean = np.random.rand(no_of_dist, 2)* scaling_factor[0]\n",
        "    stdev = np.random.rand(no_of_dist)* scaling_factor[1]\n",
        "\n",
        "    # Generate dataset\n",
        "    idx = np.random.randint(0, high = no_of_dist, size = dataset_size)\n",
        "    mean_arr = mean[idx]\n",
        "    stdev_arr = stdev[idx]\n",
        "\n",
        "    data = np.zeros((dataset_size, 2))\n",
        "    for i in range(dataset_size):\n",
        "        data[i] = get_sample(mean_arr[i], stdev_arr[i])\n",
        "    \n",
        "    return idx, data, mean, stdev\n",
        "\n",
        "# Fixed size of images : 64 x 64 x 1\n",
        "# For simplicity keep size of squares to be exponents of 2 i.e. 2, 4, 8 ...\n",
        "def draw_square(im, square_size, im_size, top_left = None):\n",
        "    if top_left is None:\n",
        "      top_left = np.random.randint(0, high = im_size - square_size, size = 2)\n",
        "    \n",
        "    # Set colour \n",
        "    im[top_left[0] : top_left[0] + square_size, top_left[1] : top_left[1] + square_size] = 1\n",
        "\n",
        "    return im, top_left\n",
        "\n",
        "def non_overlapping_square(im, no_of_squares, square_size, im_size):\n",
        "    bad_corners = True\n",
        "    while bad_corners:\n",
        "        top_left = np.random.randint(0, high = im_size - square_size - 1, size = (no_of_squares, 2))\n",
        "        new_im = np.copy(im)\n",
        "        found_combination = False\n",
        "        for i in range(0, no_of_squares):\n",
        "            relevant_square = new_im[top_left[i][0]: top_left[i][0] + square_size, top_left[i][1] : top_left[i][1] + square_size] \n",
        "            neighbors = relevant_square[relevant_square == 1]\n",
        "            if neighbors.shape[0] == 0:\n",
        "                draw_square(new_im, square_size, im_size, top_left[i])\n",
        "                if i == no_of_squares - 1:\n",
        "                    found_combination = True\n",
        "            else:\n",
        "                bad_corners = True\n",
        "                break\n",
        "\n",
        "        if bad_corners and not found_combination:\n",
        "            continue\n",
        "        bad_corners = False\n",
        "    return new_im\n",
        "\n",
        "\n",
        "\n",
        "def gen_square_image(no_of_squares, square_size, overlap, im_size):\n",
        "    im = np.zeros((im_size, im_size))\n",
        "\n",
        "    idx = []\n",
        "    if overlap:\n",
        "        for i in range(no_of_squares):\n",
        "            im, ind = draw_square(im, square_size, im_size)\n",
        "            idx.append(ind)\n",
        "    else:\n",
        "        im = non_overlapping_square(im, no_of_squares, square_size, im_size)\n",
        "\n",
        "    return im\n",
        "  \n",
        "def gen_square_dataset(no_of_squares, square_size, dataset_size, overlap, im_size = 128):\n",
        "  ds = np.zeros((dataset_size, im_size, im_size))\n",
        "  for i in range(dataset_size):\n",
        "    ds[i] = gen_square_image(no_of_squares, square_size, overlap, im_size)\n",
        "  return ds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_RgR6uPeRlq0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save dataset with 1 square of size 32 \n",
        "ds = gen_square_dataset(1, 32, 5000, False)\n",
        "np.save(\"drive/My Drive/synthetic-ds/squares/1-32.npy\", ds)\n",
        "\n",
        "# Save dataset with 4 squares of size 16 \n",
        "ds = gen_square_dataset(4, 16, 5000, False)\n",
        "np.save(\"drive/My Drive/synthetic-ds/squares/4-16.npy\", ds)\n",
        "\n",
        "# Save dataset with 10 squares of size 8 \n",
        "ds = gen_square_dataset(10, 8, 5000, False)\n",
        "np.save(\"drive/My Drive/synthetic-ds/squares/4-16.npy\", ds)\n",
        "\n",
        "# Save dataset with 10 squares of size 8 \n",
        "ds = gen_square_dataset(1, 16, 5000, False, im_size = 28)\n",
        "np.save(\"drive/My Drive/synthetic-ds/squares/1-16-28x28.npy\", ds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9ByiA9oPMIGm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Mixture of Polygons"
      ]
    },
    {
      "metadata": {
        "id": "SchDe38sMUjR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Code"
      ]
    },
    {
      "metadata": {
        "id": "ZuDcDzZpFjuz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_ct2(im_path, im_size):\n",
        "  image = Image.new('RGB', (im_size, im_size))\n",
        "  image.save(im_path, \"PNG\")\n",
        "  \n",
        "  image = Image.open(im_path)\n",
        "  draw = ImageDraw.Draw(image)\n",
        "\n",
        "  # Draw 2 circles\n",
        "  x = np.random.randint(10, high = im_size - 10)\n",
        "  y = np.random.randint(10, high = im_size - 10)\n",
        "  r = np.random.randint(min(x, y, im_size - x, im_size - y))\n",
        "  draw.ellipse((x-r, y-r, x+r, y+r), fill= (np.random.randint(256), np.random.randint(256), np.random.randint(256)))\n",
        "  \n",
        "  x = np.random.randint(10, high = im_size - 10)\n",
        "  y = np.random.randint(10, high = im_size - 10)\n",
        "  r = np.random.randint(min(x, y, im_size - x, im_size - y))\n",
        "  draw.ellipse((x-r, y-r, x+r, y+r), fill= (np.random.randint(256), np.random.randint(256), np.random.randint(256)))\n",
        "  \n",
        "  # Draw 2 triangles\n",
        "  points = ((np.random.randint(im_size), np.random.randint(im_size)), (np.random.randint(im_size), np.random.randint(im_size)), (np.random.randint(im_size), np.random.randint(im_size)))\n",
        "  draw.polygon((points), fill= (np.random.randint(256), np.random.randint(256), np.random.randint(256)))\n",
        "  \n",
        "  points = ((np.random.randint(im_size), np.random.randint(im_size)), (np.random.randint(im_size), np.random.randint(im_size)), (np.random.randint(im_size), np.random.randint(im_size)))\n",
        "  draw.polygon((points), fill= (np.random.randint(256), np.random.randint(256), np.random.randint(256)))\n",
        "\n",
        "  image.save(im_path)\n",
        "  \n",
        "def generate_ctp2(im_path, im_size):\n",
        "  # Create new image (all black)\n",
        "  image = Image.new('RGB', (im_size, im_size))\n",
        "  draw = ImageDraw.Draw(image)\n",
        "\n",
        "  # Draw 2 circles\n",
        "  x = np.random.randint(10, high = im_size - 10)\n",
        "  y = np.random.randint(10, high = im_size - 10)\n",
        "  r = np.random.randint(min(x, y, im_size - x, im_size - y))\n",
        "  draw.ellipse((x-r, y-r, x+r, y+r), fill= (np.random.randint(256), np.random.randint(256), np.random.randint(256)))\n",
        "  \n",
        "  x = np.random.randint(10, high = im_size - 10)\n",
        "  y = np.random.randint(10, high = im_size - 10)\n",
        "  r = np.random.randint(min(x, y, im_size - x, im_size - y))\n",
        "  draw.ellipse((x-r, y-r, x+r, y+r), fill= (np.random.randint(256), np.random.randint(256), np.random.randint(256)))\n",
        "  \n",
        "  # Draw 2 triangles\n",
        "  points = ((np.random.randint(im_size), np.random.randint(im_size)), (np.random.randint(im_size), np.random.randint(im_size)), (np.random.randint(im_size), np.random.randint(im_size)))\n",
        "  draw.polygon((points), fill= (np.random.randint(256), np.random.randint(256), np.random.randint(256)))\n",
        "  \n",
        "  points = ((np.random.randint(im_size), np.random.randint(im_size)), (np.random.randint(im_size), np.random.randint(im_size)), (np.random.randint(im_size), np.random.randint(im_size)))\n",
        "  draw.polygon((points), fill= (np.random.randint(256), np.random.randint(256), np.random.randint(256)))\n",
        "  \n",
        "  # Draw 2 polygons (4 sided)\n",
        "  points = ((np.random.randint(im_size), np.random.randint(im_size)), (np.random.randint(im_size), np.random.randint(im_size)), (np.random.randint(im_size), np.random.randint(im_size)), (np.random.randint(im_size), np.random.randint(im_size)))\n",
        "  draw.polygon((points), fill= (np.random.randint(256), np.random.randint(256), np.random.randint(256)))\n",
        "  \n",
        "  points = ((np.random.randint(im_size), np.random.randint(im_size)), (np.random.randint(im_size), np.random.randint(im_size)), (np.random.randint(im_size), np.random.randint(im_size)), (np.random.randint(im_size), np.random.randint(im_size)))\n",
        "  draw.polygon((points), fill= (np.random.randint(256), np.random.randint(256), np.random.randint(256)))\n",
        "\n",
        "  image.save()\n",
        "\n",
        "def gen_image_dataset(image_name, im_size, dataset_size, image_generator, path):\n",
        "  im_path = path + image_name\n",
        "  for i in range(dataset_size):\n",
        "    image_generator(im_path + str(i) + '.png', im_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KpUkIlj8Mags",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Generation"
      ]
    },
    {
      "metadata": {
        "id": "IwhmvIqDH1Xi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gen_image_dataset('image', 128, 5000, generate_ct2, 'drive/My Drive/synthetic-ds/ct2/')\n",
        "gen_image_dataset('image', 128, 5000, generate_ctp2, 'drive/My Drive/synthetic-ds/ctp2/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kgvnsALSMdCk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Utilites for testing"
      ]
    },
    {
      "metadata": {
        "id": "5VBd5tSPJnbj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Check if the files have been generated \n",
        "!ls drive/My\\ Drive/synthetic-ds/ct2/\n",
        "!ls drive/My\\ Drive/synthetic-ds/ctp2/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RvpL6q6cEwe1",
        "colab_type": "code",
        "outputId": "9867ce2b-4680-4e71-8c55-0b954b399984",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "cell_type": "code",
      "source": [
        "# Delete all files in relevant folders\n",
        "!rm drive/My\\ Drive/synthetic-ds/ct2/*\n",
        "!rm drive/My\\ Drive/synthetic-ds/ctp2/*"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'drive/My Drive/synthetic-ds/ctp2/*': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-bSwOViJTcUP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Point (in $R^n$) Dataset Generation "
      ]
    },
    {
      "metadata": {
        "id": "DX032VoVXqp8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Circles"
      ]
    },
    {
      "metadata": {
        "id": "Owc5s4aITuos",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "circle_factor_1 = datasets.make_circles(n_samples = 5000, factor = 0.1, random_state = 0)\n",
        "circle_factor_5 = datasets.make_circles(n_samples = 5000, factor = 0.5, random_state = 0)\n",
        "circle_factor_9 = datasets.make_circles(n_samples = 5000, factor = 0.9, random_state = 0)\n",
        "\n",
        "circle_noise_5 = datasets.make_circles(n_samples = 5000, factor = 0.5, noise = 0.05, random_state = 0)\n",
        "circle_noise_10 = datasets.make_circles(n_samples = 5000, factor = 0.5, noise = 0.1, random_state = 0)\n",
        "circle_noise_50 = datasets.make_circles(n_samples = 5000, factor = 0.5, noise = 0.5, random_state = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yYTMK2IuYm9e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.save('drive/My Drive/synthetic-ds/point/circles/circle_factor_1', np.column_stack((circle_factor_1[0], circle_factor_1[1])))\n",
        "np.save('drive/My Drive/synthetic-ds/point/circles/circle_factor_5', np.column_stack((circle_factor_5[0], circle_factor_5[1])))\n",
        "np.save('drive/My Drive/synthetic-ds/point/circles/circle_factor_9', np.column_stack((circle_factor_9[0], circle_factor_9[1])))\n",
        "\n",
        "np.save('drive/My Drive/synthetic-ds/point/circles/circle_noise_5', np.column_stack((circle_noise_5[0], circle_noise_5[1])))\n",
        "np.save('drive/My Drive/synthetic-ds/point/circles/circle_noise_10', np.column_stack((circle_noise_10[0], circle_noise_10[1])))\n",
        "np.save('drive/My Drive/synthetic-ds/point/circles/circle_noise_50', np.column_stack((circle_noise_50[0], circle_noise_50[1])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r263XCOZcCkK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## S Curve"
      ]
    },
    {
      "metadata": {
        "id": "qfl0YpdoaEJW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "scurve_noise_5 = datasets.make_s_curve(n_samples = 5000, noise = 0.05, random_state = 0)\n",
        "scurve_noise_10 = datasets.make_s_curve(n_samples = 5000, noise = 0.1, random_state = 0)\n",
        "scurve_noise_50 = datasets.make_s_curve(n_samples = 5000, noise = 0.5, random_state = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EskCQsSQc1wz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.save('drive/My Drive/synthetic-ds/point/s_curves/scurve_noise_5', np.column_stack((scurve_noise_5[0], scurve_noise_5[1])))\n",
        "np.save('drive/My Drive/synthetic-ds/point/s_curves/scurve_noise_10', np.column_stack((scurve_noise_10[0], scurve_noise_10[1])))\n",
        "np.save('drive/My Drive/synthetic-ds/point/s_curves/scurve_noise_50', np.column_stack((scurve_noise_50[0], scurve_noise_50[1])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NAkiRWMxdEQZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Swiss Roll"
      ]
    },
    {
      "metadata": {
        "id": "q6RaFk1cdEbG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "roll_noise_5 = datasets.make_swiss_roll(n_samples = 5000, noise = 0.05, random_state = 0)\n",
        "roll_noise_10 = datasets.make_swiss_roll(n_samples = 5000, noise = 0.1, random_state = 0)\n",
        "roll_noise_50 = datasets.make_swiss_roll(n_samples = 5000, noise = 0.5, random_state = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1pvrCclidEsO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.save('drive/My Drive/synthetic-ds/point/swiss_roll/roll_noise_5', np.column_stack((roll_noise_5[0], roll_noise_5[1])))\n",
        "np.save('drive/My Drive/synthetic-ds/point/swiss_roll/roll_noise_10', np.column_stack((roll_noise_10[0], roll_noise_10[1])))\n",
        "np.save('drive/My Drive/synthetic-ds/point/swiss_roll/roll_noise_50', np.column_stack((roll_noise_50[0], roll_noise_50[1])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ASOIQ39Ld7e2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Clusters"
      ]
    },
    {
      "metadata": {
        "id": "F8wpNwF0d7nV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1533
        },
        "outputId": "cf11a7b9-3018-4e87-f00a-1c1db38509bb"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class DCGAN():\n",
        "    def __init__(self):\n",
        "        # Input shape\n",
        "        self.img_rows = 28\n",
        "        self.img_cols = 28\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.latent_dim = 100\n",
        "\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss='binary_crossentropy',\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # The generator takes noise as input and generates imgs\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        img = self.generator(z)\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # The discriminator takes generated images as input and determines validity\n",
        "        valid = self.discriminator(img)\n",
        "\n",
        "        # The combined model  (stacked generator and discriminator)\n",
        "        # Trains the generator to fool the discriminator\n",
        "        self.combined = Model(z, valid)\n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n",
        "        model.add(Reshape((7, 7, 128)))\n",
        "        model.add(UpSampling2D())\n",
        "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(UpSampling2D())\n",
        "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n",
        "        model.add(Activation(\"tanh\"))\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        img = model(noise)\n",
        "\n",
        "        return Model(noise, img)\n",
        "\n",
        "    def build_discriminator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        validity = model(img)\n",
        "\n",
        "        return Model(img, validity)\n",
        "\n",
        "    def train(self, epochs, batch_size=128, save_interval=50):\n",
        "\n",
        "        '''\n",
        "        # Load the dataset\n",
        "        (X_train, _), (_, _) = mnist.load_data()\n",
        "\n",
        "        # Rescale -1 to 1\n",
        "        X_train = X_train / 127.5 - 1.\n",
        "        X_train = np.expand_dims(X_train, axis=3)\n",
        "        '''\n",
        "        \n",
        "        X_train = np.load('drive/My Drive/synthetic-ds/squares/1-16-28x28.npy')\n",
        "        X_train = X_train/0.5 - 1\n",
        "        X_train = np.expand_dims(X_train, axis=3)\n",
        "        \n",
        "        # Adversarial ground truths\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random half of images\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            imgs = X_train[idx]\n",
        "\n",
        "            # Sample noise and generate a batch of new images\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "            gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "            # Train the discriminator (real classified as ones and generated as zeros)\n",
        "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            # Train the generator (wants discriminator to mistake images as real)\n",
        "            g_loss = self.combined.train_on_batch(noise, valid)\n",
        "\n",
        "            \n",
        "            if epoch%1000 == 0:\n",
        "              # Plot the progress\n",
        "              print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % save_interval == 0:\n",
        "                self.save_imgs(epoch)\n",
        "\n",
        "    def save_imgs(self, epoch):\n",
        "        r, c = 5, 5\n",
        "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
        "        gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "        # Rescale images 0 - 1\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    dcgan = DCGAN()\n",
        "    dcgan.train(epochs=50000, batch_size=32, save_interval=1000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 32)        320       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 64)          18496     \n",
            "_________________________________________________________________\n",
            "zero_padding2d_1 (ZeroPaddin (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 8, 8, 64)          256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 4, 4, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 4, 4, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 4, 4, 256)         295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 4097      \n",
            "=================================================================\n",
            "Total params: 393,729\n",
            "Trainable params: 392,833\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 6272)              633472    \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2 (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 14, 14, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 14, 14, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 28, 28, 64)        73792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 28, 28, 1)         577       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 856,193\n",
            "Trainable params: 855,809\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 [D loss: 1.478233, acc.: 43.75%] [G loss: 1.763861]\n",
            "1000 [D loss: 0.005558, acc.: 100.00%] [G loss: 5.526086]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L9c-sxPUwuFJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm images/*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5EsKfqec0aM9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image(filename='test.png')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}